<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/core/edges.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/core/edges.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Edges and graph builder for the agent&#10;&quot;&quot;&quot;&#10;from typing import Callable, Dict&#10;from langgraph.graph import END, StateGraph&#10;from langgraph.graph import MessagesState&#10;from .utils import router_decision&#10;&#10;&#10;def build_graph(nodes: Dict[str, Callable[[MessagesState], dict]]):&#10;    &quot;&quot;&quot;Build and compile a MessagesState graph from provided node callables.&#10;&#10;    Expected keys in nodes: router, planner, executor, critic&#10;    Each value should be a callable(state) -&gt; dict returning partial state updates.&#10;    &quot;&quot;&quot;&#10;    graph = StateGraph(MessagesState)  # type: ignore[arg-type]&#10;&#10;    graph.add_node(&quot;router&quot;, nodes[&quot;router&quot;])&#10;    graph.add_node(&quot;planner&quot;, nodes[&quot;planner&quot;])&#10;    graph.add_node(&quot;executor&quot;, nodes[&quot;executor&quot;])&#10;    graph.add_node(&quot;critic&quot;, nodes[&quot;critic&quot;])&#10;&#10;    graph.set_entry_point(&quot;router&quot;)&#10;&#10;    # Conditional from router: executor | planner | END&#10;    graph.add_conditional_edges(&#10;        &quot;router&quot;,&#10;        router_decision,&#10;        {&quot;executor&quot;: &quot;executor&quot;, &quot;planner&quot;: &quot;planner&quot;, &quot;END&quot;: END},&#10;    )&#10;&#10;    # After executor, go back to router to allow iterative calls&#10;    graph.add_edge(&quot;executor&quot;, &quot;router&quot;)&#10;&#10;    # After planner, go to critic for finalization&#10;    graph.add_edge(&quot;planner&quot;, &quot;critic&quot;)&#10;&#10;    # Finish after critic&#10;    graph.add_edge(&quot;critic&quot;, END)&#10;&#10;    return graph.compile()&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Edges and graph builder for the agent&#10;&quot;&quot;&quot;&#10;from typing import Callable, Dict&#10;from langgraph.graph import END, StateGraph&#10;from langgraph.graph import MessagesState&#10;from .utils import router_decision&#10;&#10;&#10;def build_graph(nodes: Dict[str, Callable[[MessagesState], dict]]):&#10;    &quot;&quot;&quot;Build and compile a MessagesState graph from provided node callables.&#10;&#10;    Expected keys in nodes: router, planner, executor, critic&#10;    Each value should be a callable(state) -&gt; dict returning partial state updates.&#10;    &quot;&quot;&quot;&#10;    graph = StateGraph(MessagesState)  # type: ignore[arg-type]&#10;&#10;    graph.add_node(&quot;router&quot;, nodes[&quot;router&quot;])&#10;    graph.add_node(&quot;planner&quot;, nodes[&quot;planner&quot;])&#10;    graph.add_node(&quot;executor&quot;, nodes[&quot;executor&quot;])&#10;    graph.add_node(&quot;critic&quot;, nodes[&quot;critic&quot;])&#10;&#10;    graph.set_entry_point(&quot;router&quot;)&#10;&#10;    # Router can branch to executor | planner | critic | END&#10;    graph.add_conditional_edges(&#10;        &quot;router&quot;,&#10;        router_decision,&#10;        {&quot;executor&quot;: &quot;executor&quot;, &quot;planner&quot;: &quot;planner&quot;, &quot;critic&quot;: &quot;critic&quot;, &quot;END&quot;: END},&#10;    )&#10;&#10;    # All work nodes return to router; router decides to continue or END&#10;    graph.add_edge(&quot;executor&quot;, &quot;router&quot;)&#10;    graph.add_edge(&quot;planner&quot;, &quot;router&quot;)&#10;    graph.add_edge(&quot;critic&quot;, &quot;router&quot;)&#10;&#10;    return graph.compile()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/core/engine.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/core/engine.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Core components that graph is compiled&#10;&quot;&quot;&quot;&#10;from typing import Dict, List, Optional&#10;&#10;from langchain_core.messages import AnyMessage, HumanMessage&#10;&#10;from .models import Models&#10;from .edges import build_graph&#10;from .nodes import router_node, planner_node, executor_node, critical_node&#10;from langchain_core.tools import BaseTool&#10;from core import config as core_config&#10;&#10;&#10;class Engine:&#10;    &quot;&quot;&quot;Engine that wires models, tools, and the graph, and exposes a run() method.&quot;&quot;&quot;&#10;&#10;    def __init__(self, tools: Optional[List[BaseTool]] = None):&#10;        # Initialize models using singleton core.config&#10;        self._models = Models(core_config)&#10;        self._router_llm = self._models.getRouterModel()&#10;        self._planner_llm = self._models.getPlannerModel()&#10;        self._executor_llm = self._models.getExecutorModel()&#10;        self._critic_llm = self._models.getCriticModel()&#10;&#10;        # Tools to bind and execute&#10;        self._tools: List[BaseTool] = tools or []&#10;&#10;        # Bind tools to all models if supported&#10;        for attr in (&quot;_router_llm&quot;, &quot;_planner_llm&quot;, &quot;_executor_llm&quot;, &quot;_critic_llm&quot;):&#10;            llm = getattr(self, attr)&#10;            try:&#10;                if self._tools:&#10;                    setattr(self, attr, llm.bind_tools(self._tools))&#10;            except AttributeError:&#10;                # If underlying model doesn't support bind_tools, ignore binding&#10;                pass&#10;&#10;        # Build graph with closures capturing LLMs and tools&#10;        self.app = build_graph(&#10;            {&#10;                &quot;router&quot;: lambda state: router_node(state, self._router_llm),&#10;                &quot;planner&quot;: lambda state: planner_node(state, self._planner_llm),&#10;                &quot;executor&quot;: lambda state: executor_node(state, self._executor_llm, self._tools),&#10;                &quot;critic&quot;: lambda state: critical_node(state, self._critic_llm),&#10;            }&#10;        )&#10;&#10;    def run(self, user_input: str, history: Optional[List[AnyMessage]] = None) -&gt; Dict[str, List[AnyMessage]]:&#10;        &quot;&quot;&quot;Run the agent graph once for a given user input and return updated messages.&#10;&#10;        Args:&#10;            user_input: The user's message content.&#10;            history: Optional previous messages to preserve conversation context.&#10;        Returns:&#10;            A dict with key &quot;messages&quot; containing the updated list of messages.&#10;        &quot;&quot;&quot;&#10;        messages: List[AnyMessage] = list(history or []) + [HumanMessage(content=user_input)]&#10;        result = self.app.invoke({&quot;messages&quot;: messages})&#10;        return result&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Core components that graph is compiled&#10;&quot;&quot;&quot;&#10;from typing import Dict, List, Optional&#10;&#10;from langchain_core.messages import AnyMessage, HumanMessage&#10;&#10;from .models import Models&#10;from .edges import build_graph&#10;from .nodes import router_node, planner_node, executor_node, critical_node&#10;from langchain_core.tools import BaseTool&#10;from core import config as core_config&#10;&#10;&#10;class Engine:&#10;    &quot;&quot;&quot;Engine that wires models, tools, and the graph, and exposes a run() method.&quot;&quot;&quot;&#10;&#10;    def __init__(self, tools: Optional[List[BaseTool]] = None):&#10;        # Initialize models using singleton core.config&#10;        self._models = Models(core_config)&#10;        self._router_llm = self._models.getRouterModel()&#10;        self._planner_llm = self._models.getPlannerModel()&#10;        self._executor_llm = self._models.getExecutorModel()&#10;        self._critic_llm = self._models.getCriticModel()&#10;&#10;        # Tools to bind and execute&#10;        self._tools: List[BaseTool] = tools or []&#10;&#10;        # Bind tools to all models if supported&#10;        for attr in (&quot;_router_llm&quot;, &quot;_planner_llm&quot;, &quot;_executor_llm&quot;, &quot;_critic_llm&quot;):&#10;            llm = getattr(self, attr)&#10;            try:&#10;                if self._tools:&#10;                    setattr(self, attr, llm.bind_tools(self._tools))&#10;            except AttributeError:&#10;                # If underlying model doesn't support bind_tools, ignore binding&#10;                pass&#10;&#10;        # Build graph with closures capturing LLMs and tools&#10;        self.app = build_graph(&#10;            {&#10;                &quot;router&quot;: lambda state: router_node(state, self._router_llm),&#10;                &quot;planner&quot;: lambda state: planner_node(state, self._planner_llm),&#10;                &quot;executor&quot;: lambda state: executor_node(state, self._executor_llm, self._tools),&#10;                &quot;critic&quot;: lambda state: critical_node(state, self._critic_llm),&#10;            }&#10;        )&#10;&#10;    def run(self, user_input: str, history: Optional[List[AnyMessage]] = None) -&gt; Dict[str, List[AnyMessage]]:&#10;        &quot;&quot;&quot;Run the agent graph once for a given user input and return updated messages.&#10;&#10;        Args:&#10;            user_input: The user's message content.&#10;            history: Optional previous messages to preserve conversation context.&#10;        Returns:&#10;            A dict with key &quot;messages&quot; containing the updated list of messages.&#10;        &quot;&quot;&quot;&#10;        messages: List[AnyMessage] = list(history or []) + [HumanMessage(content=user_input)]&#10;        result = self.app.invoke({&quot;messages&quot;: messages})&#10;        return result" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/core/nodes.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/core/nodes.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;All the nodes in the graph that is needed&#10;&quot;&quot;&quot;&#10;from typing import Dict, List, Optional&#10;from langchain_core.messages import (&#10;    AnyMessage,&#10;    AIMessage,&#10;    ToolMessage,&#10;)&#10;from langchain_openai import ChatOpenAI&#10;from langgraph.graph import MessagesState&#10;from langchain_core.tools import BaseTool&#10;from .utils import ensure_system&#10;from core import config&#10;&#10;&#10;def _execute_tool_calls(ai_msg: AIMessage, tools_map: Dict[str, BaseTool]) -&gt; List[ToolMessage]:&#10;    &quot;&quot;&quot;Execute tool calls from an AI message and return ToolMessage list.&quot;&quot;&quot;&#10;    results: List[ToolMessage] = []&#10;    for call in getattr(ai_msg, &quot;tool_calls&quot;, []) or []:&#10;        name = call.get(&quot;name&quot;) if isinstance(call, dict) else getattr(call, &quot;name&quot;, None)&#10;        args = call.get(&quot;args&quot;, {}) if isinstance(call, dict) else getattr(call, &quot;args&quot;, {})&#10;        call_id = call.get(&quot;id&quot;) if isinstance(call, dict) else getattr(call, &quot;id&quot;, None)&#10;        tool = tools_map.get(name)&#10;        if tool is None:&#10;            results.append(ToolMessage(content=f&quot;Tool '{name}' not found.&quot;, tool_call_id=call_id))&#10;            continue&#10;        try:&#10;            # BaseTool.invoke expects a dict or str depending on schema&#10;            output = tool.invoke(args)&#10;        except Exception as e:&#10;            output = f&quot;Tool '{name}' error: {e}&quot;&#10;        results.append(ToolMessage(content=str(output), tool_call_id=call_id))&#10;    return results&#10;&#10;&#10;def router_node(state: MessagesState, llm: ChatOpenAI):&#10;    &quot;&quot;&quot;&#10;    Router node: decide next step and optionally propose tool calls.&#10;    :param state: Current MessageState&#10;    :param llm: Model bound with tools&#10;    :return: Dict&#10;    &quot;&quot;&quot;&#10;    messages = ensure_system(state[&quot;messages&quot;], override_prompt=config.ROUTER_SYSTEM_PROMPT)&#10;    ai_msg = llm.invoke(messages)&#10;    return {&quot;messages&quot;: messages + [ai_msg]}&#10;&#10;&#10;def planner_node(state: MessagesState, llm: ChatOpenAI):&#10;    &quot;&quot;&quot;&#10;    Planner node: produce a plan or next reasoning step.&#10;    &quot;&quot;&quot;&#10;    messages = ensure_system(state[&quot;messages&quot;], override_prompt=config.PLANNER_SYSTEM_PROMPT)&#10;    ai_msg = llm.invoke(messages)&#10;    return {&quot;messages&quot;: messages + [ai_msg]}&#10;&#10;&#10;def executor_node(state: MessagesState, llm: ChatOpenAI, tools: Optional[List[BaseTool]] = None):&#10;    &quot;&quot;&quot;&#10;    Executor node: execute tools requested by the last AI message and append ToolMessages.&#10;    &quot;&quot;&quot;&#10;    messages: List[AnyMessage] = ensure_system(state[&quot;messages&quot;], override_prompt=config.EXECUTOR_SYSTEM_PROMPT)&#10;    tools_map: Dict[str, BaseTool] = {t.name: t for t in (tools or [])}&#10;    last_ai = messages[-1] if messages else None&#10;    tool_msgs: List[ToolMessage] = []&#10;    if isinstance(last_ai, AIMessage) and getattr(last_ai, &quot;tool_calls&quot;, None):&#10;        tool_msgs = _execute_tool_calls(last_ai, tools_map)&#10;    # After execution, return messages with tool outputs so LLM can continue&#10;    return {&quot;messages&quot;: messages + tool_msgs}&#10;&#10;&#10;def critical_node(state: MessagesState, llm: ChatOpenAI):&#10;    &quot;&quot;&quot;&#10;    Critic Node: perform a final check or provide final answer.&#10;    &quot;&quot;&quot;&#10;    messages = ensure_system(state[&quot;messages&quot;], override_prompt=config.CRITIC_SYSTEM_PROMPT)&#10;    ai_msg = llm.invoke(messages)&#10;    return {&quot;messages&quot;: messages + [ai_msg]}&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;All the nodes in the graph that is needed&#10;&quot;&quot;&quot;&#10;from typing import Dict, List, Optional&#10;from langchain_core.messages import (&#10;    AnyMessage,&#10;    AIMessage,&#10;    ToolMessage,&#10;)&#10;from langchain_openai import ChatOpenAI&#10;from langgraph.graph import MessagesState&#10;from langchain_core.tools import BaseTool&#10;from .utils import ensure_system&#10;from core import config&#10;&#10;&#10;def _execute_tool_calls(ai_msg: AIMessage, tools_map: Dict[str, BaseTool]) -&gt; List[ToolMessage]:&#10;    &quot;&quot;&quot;Execute tool calls from an AI message and return ToolMessage list.&quot;&quot;&quot;&#10;    results: List[ToolMessage] = []&#10;    for call in getattr(ai_msg, &quot;tool_calls&quot;, []) or []:&#10;        name = call.get(&quot;name&quot;) if isinstance(call, dict) else getattr(call, &quot;name&quot;, None)&#10;        args = call.get(&quot;args&quot;, {}) if isinstance(call, dict) else getattr(call, &quot;args&quot;, {})&#10;        call_id = call.get(&quot;id&quot;) if isinstance(call, dict) else getattr(call, &quot;id&quot;, None)&#10;        tool = tools_map.get(name)&#10;        if tool is None:&#10;            results.append(ToolMessage(content=f&quot;Tool '{name}' not found.&quot;, tool_call_id=call_id))&#10;            continue&#10;        try:&#10;            # BaseTool.invoke expects a dict or str depending on schema&#10;            output = tool.invoke(args)&#10;        except Exception as e:&#10;            output = f&quot;Tool '{name}' error: {e}&quot;&#10;        results.append(ToolMessage(content=str(output), tool_call_id=call_id))&#10;    return results&#10;&#10;&#10;def router_node(state: MessagesState, llm: ChatOpenAI):&#10;    &quot;&quot;&quot;&#10;    Router node: decide next step and optionally propose tool calls.&#10;    :param state: Current MessageState&#10;    :param llm: Model bound with tools&#10;    :return: Dict&#10;    &quot;&quot;&quot;&#10;    messages = ensure_system(state[&quot;messages&quot;], override_prompt=config.ROUTER_SYSTEM_PROMPT)&#10;    ai_msg = llm.invoke(messages)&#10;    return {&quot;messages&quot;: messages + [ai_msg]}&#10;&#10;&#10;def planner_node(state: MessagesState, llm: ChatOpenAI):&#10;    &quot;&quot;&quot;&#10;    Planner node: produce a plan or next reasoning step.&#10;    &quot;&quot;&quot;&#10;    messages = ensure_system(state[&quot;messages&quot;], override_prompt=config.PLANNER_SYSTEM_PROMPT)&#10;    ai_msg = llm.invoke(messages)&#10;    return {&quot;messages&quot;: messages + [ai_msg]}&#10;&#10;&#10;def executor_node(state: MessagesState, llm: ChatOpenAI, tools: Optional[List[BaseTool]] = None):&#10;    &quot;&quot;&quot;&#10;    Executor node: execute tools requested by the last AI message and append ToolMessages.&#10;    &quot;&quot;&quot;&#10;    messages: List[AnyMessage] = ensure_system(state[&quot;messages&quot;], override_prompt=config.EXECUTOR_SYSTEM_PROMPT)&#10;    tools_map: Dict[str, BaseTool] = {t.name: t for t in (tools or [])}&#10;    last_ai = messages[-1] if messages else None&#10;    tool_msgs: List[ToolMessage] = []&#10;    if isinstance(last_ai, AIMessage) and getattr(last_ai, &quot;tool_calls&quot;, None):&#10;        tool_msgs = _execute_tool_calls(last_ai, tools_map)&#10;    # After execution, return messages with tool outputs so LLM can continue&#10;    return {&quot;messages&quot;: messages + tool_msgs}&#10;&#10;&#10;def critical_node(state: MessagesState, llm: ChatOpenAI):&#10;    &quot;&quot;&quot;&#10;    Critic Node: perform a final check or provide final answer.&#10;    &quot;&quot;&quot;&#10;    messages = ensure_system(state[&quot;messages&quot;], override_prompt=config.CRITIC_SYSTEM_PROMPT)&#10;    ai_msg = llm.invoke(messages)&#10;    return {&quot;messages&quot;: messages + [ai_msg]}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/core/plugin/ToolBindings.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/core/plugin/ToolBindings.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Bindings for all tools, which the agent engine would use that&#10;&quot;&quot;&quot;&#10;from typing import Iterable, List, Union&#10;from langchain_core.tools import BaseTool&#10;&#10;class ToolBindings:&#10;    &quot;&quot;&quot;&#10;    This class holds a list of tools.&#10;    Design:&#10;      - Each model (router/planner/...) may have its own tool bindings in future.&#10;      - User added tools are available to models that support tools (e.g., router).&#10;    &quot;&quot;&quot;&#10;    tools: List[BaseTool]&#10;&#10;    def __init__(self):&#10;        self.tools = []&#10;&#10;    def addTool(self, tools: Union[BaseTool, List[BaseTool]]):&#10;        &quot;&quot;&quot;Add a tool or a list of tools.&quot;&quot;&quot;&#10;        if tools is None:&#10;            return&#10;        if isinstance(tools, list):&#10;            self.tools.extend(tools)&#10;        else:&#10;            self.tools.append(tools)&#10;&#10;    def getTools(self) -&gt; List[BaseTool]:&#10;        return self.tools" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Bindings for all tools, which the agent engine would use that&#10;&quot;&quot;&quot;&#10;from typing import Iterable, List, Union&#10;from langchain_core.tools import BaseTool&#10;&#10;class ToolBindings:&#10;    &quot;&quot;&quot;&#10;    This class holds a list of tools.&#10;    Design:&#10;      - Each model (router/planner/...) may have its own tool bindings in future.&#10;      - User added tools are available to models that support tools (e.g., router).&#10;    &quot;&quot;&quot;&#10;    tools: List[BaseTool]&#10;&#10;    def __init__(self):&#10;        self.tools = []&#10;&#10;    def addTool(self, tools: Union[BaseTool, List[BaseTool]]):&#10;        &quot;&quot;&quot;Add a tool or a list of tools.&quot;&quot;&quot;&#10;        if tools is None:&#10;            return&#10;        if isinstance(tools, list):&#10;            self.tools.extend(tools)&#10;        else:&#10;            self.tools.append(tools)&#10;&#10;    def getTools(self) -&gt; List[BaseTool]:&#10;        return self.tools" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/core/utils.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/core/utils.py" />
              <option name="originalContent" value="from typing import List&#10;&#10;from langchain_core.messages import AnyMessage, SystemMessage, AIMessage&#10;from langgraph.graph import MessagesState&#10;&#10;from core import config&#10;&#10;&#10;def ensure_system(messages: List[AnyMessage], override_prompt: str | None = None):&#10;    &quot;&quot;&quot;Ensure System Prompt is injected; allow override per node.&quot;&quot;&quot;&#10;    prompt = (override_prompt or config.SYSTEM_PROMPT or &quot;You are Aurora Agent.&quot;)&#10;    if messages and isinstance(messages[0], SystemMessage):&#10;        return messages&#10;    return [SystemMessage(content=prompt)] + messages&#10;&#10;&#10;def has_tool_calls(state: MessagesState) -&gt; bool:&#10;    &quot;&quot;&quot;Return True if the last AI message contains tool calls.&quot;&quot;&quot;&#10;    msgs = state.get(&quot;messages&quot;, [])&#10;    if not msgs:&#10;        return False&#10;    last = msgs[-1]&#10;    if isinstance(last, AIMessage) and getattr(last, &quot;tool_calls&quot;, None):&#10;        return True&#10;    return False&#10;&#10;&#10;def _truthy(val: str | None) -&gt; bool:&#10;    return str(val).strip().lower() in {&quot;1&quot;, &quot;true&quot;, &quot;yes&quot;, &quot;y&quot;}&#10;&#10;&#10;def router_decision(state: MessagesState) -&gt; str:&#10;    &quot;&quot;&quot;Return next route from 'router': executor | planner | critic | END.&#10;    Rule:&#10;      - If AI proposed tool calls -&gt; executor&#10;      - Else if ROUTER_FORCE_PLANNER truthy -&gt; planner&#10;      - Else if ROUTER_FORCE_CRITIC truthy -&gt; critic&#10;      - Else -&gt; END&#10;    &quot;&quot;&quot;&#10;    if has_tool_calls(state):&#10;        return &quot;executor&quot;&#10;    if _truthy(getattr(config, &quot;ROUTER_FORCE_PLANNER&quot;, &quot;&quot;)):&#10;        return &quot;planner&quot;&#10;    if _truthy(getattr(config, &quot;ROUTER_FORCE_CRITIC&quot;, &quot;&quot;)):&#10;        return &quot;critic&quot;&#10;    return &quot;END&quot;&#10;&#10;&#10;# Backward compatibility for older mapping&#10;&#10;def need_tools(state: MessagesState) -&gt; str:&#10;    return &quot;executor&quot; if has_tool_calls(state) else &quot;planner&quot;&#10;" />
              <option name="updatedContent" value="from typing import List&#10;&#10;from langchain_core.messages import AnyMessage, SystemMessage, AIMessage&#10;from langgraph.graph import MessagesState&#10;&#10;from core import config&#10;&#10;&#10;def ensure_system(messages: List[AnyMessage], override_prompt: str | None = None):&#10;    &quot;&quot;&quot;Ensure System Prompt is injected; allow override per node.&quot;&quot;&quot;&#10;    prompt = (override_prompt or config.SYSTEM_PROMPT or &quot;You are Aurora Agent.&quot;)&#10;    if messages and isinstance(messages[0], SystemMessage):&#10;        return messages&#10;    return [SystemMessage(content=prompt)] + messages&#10;&#10;&#10;def has_tool_calls(state: MessagesState) -&gt; bool:&#10;    &quot;&quot;&quot;Return True if the last AI message contains tool calls.&quot;&quot;&quot;&#10;    msgs = state.get(&quot;messages&quot;, [])&#10;    if not msgs:&#10;        return False&#10;    last = msgs[-1]&#10;    if isinstance(last, AIMessage) and getattr(last, &quot;tool_calls&quot;, None):&#10;        return True&#10;    return False&#10;&#10;&#10;def _truthy(val: str | None) -&gt; bool:&#10;    return str(val).strip().lower() in {&quot;1&quot;, &quot;true&quot;, &quot;yes&quot;, &quot;y&quot;}&#10;&#10;&#10;def router_decision(state: MessagesState) -&gt; str:&#10;    &quot;&quot;&quot;Return next route from 'router': executor | planner | critic | END.&#10;    Rule:&#10;      - If AI proposed tool calls -&gt; executor&#10;      - Else if ROUTER_FORCE_PLANNER truthy -&gt; planner&#10;      - Else if ROUTER_FORCE_CRITIC truthy -&gt; critic&#10;      - Else -&gt; END&#10;    &quot;&quot;&quot;&#10;    if has_tool_calls(state):&#10;        return &quot;executor&quot;&#10;    if _truthy(getattr(config, &quot;ROUTER_FORCE_PLANNER&quot;, &quot;&quot;)):&#10;        return &quot;planner&quot;&#10;    if _truthy(getattr(config, &quot;ROUTER_FORCE_CRITIC&quot;, &quot;&quot;)):&#10;        return &quot;critic&quot;&#10;    return &quot;END&quot;&#10;&#10;&#10;# Backward compatibility for older mapping&#10;&#10;def need_tools(state: MessagesState) -&gt; str:&#10;    return &quot;executor&quot; if has_tool_calls(state) else &quot;planner&quot;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>